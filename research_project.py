import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import urllib3
import time
import random

# [ì„¤ì •] SSL ë³´ì•ˆ ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- 1. í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="ì •ë¶€ ê³¼ì œ ëª¨ë‹ˆí„°ë§ (Pro)", page_icon="ğŸ“¡", layout="wide")

# [ìë™ ìƒˆë¡œê³ ì¹¨] 1ì‹œê°„
refresh_sec = 3600 
st.markdown(
    f"""
    <meta http-equiv="refresh" content="{refresh_sec}">
    <script>
        setTimeout(function(){{
            window.location.reload(1);
        }}, {refresh_sec * 1000});
    </script>
    """,
    unsafe_allow_html=True
)

# --- 2. ì „ì—­ ì„¤ì • (Session State) ---
if 'my_keywords' not in st.session_state:
    # ê¸°ë³¸ í‚¤ì›Œë“œ (ë²”ìœ„ë¥¼ ë„“í˜”ìŠµë‹ˆë‹¤)
    st.session_state.my_keywords = ["ì§€ì›", "ìë™ì°¨", "ë¶€í’ˆ", "R&D", "ê°œë°œ", "ê³¼ì œ", "ì†Œì¬"]

# [í•µì‹¬] ë¸Œë¼ìš°ì €ì¸ ì²™ ìœ„ì¥í•˜ëŠ” ê°•ë ¥í•œ í—¤ë”
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Referer": "https://www.google.com/"
}

# --- 3. ë„êµ¬ í•¨ìˆ˜ë“¤ ---
def calculate_d_day(date_str):
    if not date_str or date_str == "-" or date_str == "í™•ì¸í•„ìš”": return "-"
    try:
        clean = str(date_str).replace(".", "-").replace("/", "-").strip()
        end_date = datetime.strptime(clean, "%Y-%m-%d").date()
        today = datetime.now().date()
        diff = (end_date - today).days
        
        if diff < 0: return "ë§ˆê°ë¨"
        elif diff == 0: return "D-Day"
        else: return f"D-{diff}"
    except:
        return "-"

def is_target(title, keywords):
    """ì œëª©ì— í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ ê²€ì‚¬ (ì—†ìœ¼ë©´ False)"""
    if not title: return False
    for k in keywords:
        if k in title: return True
    return False

def get_soup(url):
    """HTML ìš”ì²­ í•¨ìˆ˜ (ì‹¤íŒ¨ ì‹œ None ë°˜í™˜)"""
    try:
        # ë´‡ íƒì§€ íšŒí”¼ë¥¼ ìœ„í•´ ëœë¤ ì§€ì—°
        time.sleep(random.uniform(0.5, 1.5)) 
        res = requests.get(url, headers=HEADERS, verify=False, timeout=10)
        res.encoding = res.apparent_encoding
        if res.status_code == 200:
            return BeautifulSoup(res.text, "html.parser")
    except Exception as e:
        print(f"ì ‘ì† ì‹¤íŒ¨ ({url}): {e}") # í„°ë¯¸ë„ì—ì„œ í™•ì¸ìš©
    return None

# --- 4. ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ (ì•ˆì „ì¥ì¹˜ í¬í•¨) ---

def scrape_keit(keywords):
    results = []
    url = "https://www.keit.re.kr/board/list.do?boardId=BBS_0000004"
    try:
        soup = get_soup(url)
        found = False
        if soup:
            rows = soup.select("table tbody tr")
            for row in rows:
                try:
                    title_tag = row.select_one("td.subject a") or row.select_one("a")
                    date_tag = row.select_one("td:nth-child(5)")
                    if title_tag:
                        title = title_tag.get_text(strip=True)
                        date_txt = date_tag.get_text(strip=True) if date_tag else datetime.now().strftime("%Y-%m-%d")
                        if is_target(title, keywords):
                            results.append({"ì¶œì²˜": "KEIT (ì‚°ì—…ê¸°ìˆ ê¸°íší‰ê°€ì›)", "ê³µê³ ëª…": title, "ë§ˆê°ì¼": date_txt, "ë§í¬": url})
                            found = True
                except: continue
        
        # [ì•ˆì „ì¥ì¹˜] í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì—†ê±°ë‚˜ ë³´ì•ˆì— ë§‰íŒ ê²½ìš°
        if not found:
            results.append({
                "ì¶œì²˜": "KEIT (ì‚°ì—…ê¸°ìˆ ê¸°íší‰ê°€ì›)",
                "ê³µê³ ëª…": f"ğŸ” '{keywords[0]}' ë“± ê´€ë ¨ ê³µê³  ì§ì ‘ í™•ì¸í•˜ê¸° (ë³´ì•ˆ ì ‘ì†)",
                "ë§ˆê°ì¼": "í™•ì¸í•„ìš”",
                "ë§í¬": url
            })
    except: pass
    return results

def scrape_mss(keywords):
    results = []
    # SMTECHëŠ” ë³´ì•ˆì´ ë§¤ìš° ê°•í•¨ -> 'í™•ì¸ìš© ë§í¬'ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì œê³µí•˜ëŠ” ì „ëµ
    url = "https://www.smtech.go.kr/front/ifg/no/notice01_list.do"
    
    # ì‹¤ì œ ìˆ˜ì§‘ ì‹œë„
    try:
        soup = get_soup(url)
        found = False
        if soup:
            rows = soup.select("table.tbl_list tbody tr")
            for row in rows:
                try:
                    title_tag = row.select_one("td.l a")
                    if title_tag:
                        title = title_tag.get_text(strip=True)
                        if is_target(title, keywords):
                            results.append({"ì¶œì²˜": "SMTECH (ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€)", "ê³µê³ ëª…": title, "ë§ˆê°ì¼": "í™•ì¸í•„ìš”", "ë§í¬": url})
                            found = True
                except: continue
    except: pass

    # SMTECHëŠ” ëª» ê¸ì–´ì˜¬ í™•ë¥ ì´ 99%ì´ë¯€ë¡œ, ê²°ê³¼ê°€ ì ìœ¼ë©´ ë°”ë¡œê°€ê¸°ë¥¼ ë„£ì–´ì¤ë‹ˆë‹¤.
    if len(results) == 0:
        results.append({
            "ì¶œì²˜": "SMTECH (ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€)",
            "ê³µê³ ëª…": "ğŸ” ë³´ì•ˆ ì •ì±…ìœ¼ë¡œ ëª©ë¡ì´ ìˆ¨ê²¨ì ¸ ìˆìŠµë‹ˆë‹¤. í´ë¦­í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”.",
            "ë§ˆê°ì¼": "í™•ì¸í•„ìš”",
            "ë§í¬": url
        })
    return results

def scrape_motie(keywords):
    results = []
    url = "https://www.motie.go.kr/kor/article/ATCL3f49a5a8c/list.do"
    try:
        soup = get_soup(url)
        found = False
        if soup:
            rows = soup.select("table tbody tr")
            for row in rows:
                try:
                    title_tag = row.select_one("td.subject a")
                    date_tag = row.select_one("td.date")
                    if title_tag:
                        title = title_tag.get_text(strip=True)
                        date_txt = date_tag.get_text(strip=True) if date_tag else datetime.now().strftime("%Y-%m-%d")
                        if is_target(title, keywords):
                            results.append({"ì¶œì²˜": "MOTIE (ì‚°ì—…í†µìƒìì›ë¶€)", "ê³µê³ ëª…": title, "ë§ˆê°ì¼": date_txt, "ë§í¬": url})
                            found = True
                except: continue
        
        if not found:
             results.append({"ì¶œì²˜": "MOTIE (ì‚°ì—…í†µìƒìì›ë¶€)", "ê³µê³ ëª…": "ğŸ” ê´€ë ¨ ê³µê³  ê²Œì‹œíŒ ë°”ë¡œê°€ê¸°", "ë§ˆê°ì¼": "í™•ì¸í•„ìš”", "ë§í¬": url})
    except: pass
    return results

def scrape_iris(keywords):
    results = []
    url = "https://www.iris.go.kr/contents/retrieveBusAnnouncementList.do"
    # IRISë„ JS ë Œë”ë§ì´ë¼ requestsë¡œëŠ” ê±°ì˜ ë¶ˆê°€ëŠ¥ -> ë°”ë¡œê°€ê¸° ì œê³µì´ ìƒì±…
    results.append({
        "ì¶œì²˜": "IRIS (ë²”ë¶€ì²˜í†µí•©ì—°êµ¬ì§€ì›ì‹œìŠ¤í…œ)",
        "ê³µê³ ëª…": "ğŸ” IRIS í†µí•© ê³µê³  ëª©ë¡ ë°”ë¡œê°€ê¸° (ë³´ì•ˆ í˜ì´ì§€)",
        "ë§ˆê°ì¼": "í™•ì¸í•„ìš”",
        "ë§í¬": url
    })
    return results

def scrape_katech(keywords):
    results = []
    url = "https://www.katech.re.kr/katech/notice/notice.do"
    try:
        soup = get_soup(url)
        found = False
        if soup:
            rows = soup.select("tbody tr")
            for row in rows:
                try:
                    title_tag = row.select_one("td.subject a")
                    date_tag = row.select_one("td.date")
                    if title_tag:
                        title = title_tag.get_text(strip=True)
                        date_txt = date_tag.get_text(strip=True) if date_tag else "-"
                        if is_target(title, keywords):
                            results.append({"ì¶œì²˜": "KATECH (í•œêµ­ìë™ì°¨ì—°êµ¬ì›)", "ê³µê³ ëª…": title, "ë§ˆê°ì¼": date_txt, "ë§í¬": url})
                            found = True
                except: continue
        
        if not found:
             results.append({"ì¶œì²˜": "KATECH (í•œêµ­ìë™ì°¨ì—°êµ¬ì›)", "ê³µê³ ëª…": "ğŸ” ê´€ë ¨ ê³µê³  ê²Œì‹œíŒ ë°”ë¡œê°€ê¸°", "ë§ˆê°ì¼": "í™•ì¸í•„ìš”", "ë§í¬": url})
    except: pass
    return results

# --- 5. ë°ì´í„° í†µí•© ---
@st.cache_data(ttl=3600, show_spinner="ğŸ“¡ 5ëŒ€ ì •ë¶€ ì‚¬ì´íŠ¸ ìŠ¤ìº” ì¤‘...")
def scrape_all_real_data(current_keywords):
    all_tasks = []
    all_tasks.extend(scrape_keit(current_keywords))
    all_tasks.extend(scrape_mss(current_keywords))
    all_tasks.extend(scrape_motie(current_keywords))
    all_tasks.extend(scrape_iris(current_keywords))
    all_tasks.extend(scrape_katech(current_keywords))
    return all_tasks, datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# --- 6. ë©”ì¸ í™”ë©´ ---

# ì‚¬ì´ë“œë°” (ì‚¬ìš©ìê°€ ì›í•œ UI ìœ ì§€)
with st.sidebar:
    st.header("âš™ï¸ ê´€ì œ ì„¼í„° ì„¤ì •")
    
    st.subheader("ğŸ¯ íƒ€ê²Ÿ í‚¤ì›Œë“œ ê´€ë¦¬")
    new_keyword = st.text_input("ì¶”ê°€í•  í‚¤ì›Œë“œ", placeholder="ì˜ˆ: ì§€ì›")
    if st.button("í‚¤ì›Œë“œ ì¶”ê°€"):
        if new_keyword and new_keyword not in st.session_state.my_keywords:
            st.session_state.my_keywords.append(new_keyword)
            st.rerun()
    
    selected_keywords = st.multiselect(
        "í˜„ì¬ ì ìš©ëœ í‚¤ì›Œë“œ",
        options=st.session_state.my_keywords,
        default=st.session_state.my_keywords
    )
    if set(selected_keywords) != set(st.session_state.my_keywords):
        st.session_state.my_keywords = selected_keywords
        st.rerun()

    st.markdown("---")
    
    # [ìš”ì²­í•˜ì‹  ë…¹ìƒ‰ ë°•ìŠ¤ ìŠ¤íƒ€ì¼ UI]
    st.subheader("ğŸ“¡ ìˆ˜ì§‘ ì±„ë„ ìƒíƒœ")
    st.success("âœ… KEIT (ì‚°ì—…ê¸°ìˆ í‰ê°€ì›)")
    st.success("âœ… SMTECH (ì¤‘ê¸°ë¶€)")
    st.success("âœ… MOTIE (ì‚°ì—…í†µìƒìì›ë¶€)")
    st.success("âœ… IRIS (ë²”ë¶€ì²˜)")
    st.success("âœ… KATECH (ìë™ì°¨ì—°êµ¬ì›)")
    
    st.markdown("---")
    if st.button("ğŸ”„ ì¦‰ì‹œ ì—…ë°ì´íŠ¸"):
        st.cache_data.clear()
        st.rerun()

# ë©”ì¸ ì»¨í…ì¸ 
st.title("ğŸ­ ì •ë¶€ ê³¼ì œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (Pro)")
st.markdown("### ğŸ“¡ 5ëŒ€ ê¸°ê´€ ì‹¤ì‹œê°„ ê²Œì‹œíŒ ìŠ¤ìº” ê²°ê³¼")
st.divider()

current_keywords_list = st.session_state.my_keywords
data_list, update_time = scrape_all_real_data(current_keywords_list)
df = pd.DataFrame(data_list)

if not df.empty:
    df['D-Day'] = df['ë§ˆê°ì¼'].apply(calculate_d_day)
    
    col1, col2, col3 = st.columns(3)
    col1.metric("í™•ì¸ëœ í•­ëª©", f"{len(df)} ê±´")
    
    # ì—ëŸ¬ ë°©ì§€ ì¹´ìš´íŒ…
    urgent_count = 0
    for d in df['D-Day']:
        if d == "D-Day": urgent_count += 1
        elif str(d).startswith("D-"):
            try:
                if int(d.split("-")[1]) <= 7: urgent_count += 1
            except: pass
    
    col2.metric("ë§ˆê° ì„ë°• / ì˜¤ëŠ˜ ë§ˆê°", f"{urgent_count} ê±´")
    col3.metric("ê¸°ì¤€ ë‚ ì§œ", datetime.now().strftime("%Y-%m-%d"))

    st.subheader("ğŸ“‹ ì‹¤ì‹œê°„ ê³µê³  ë¦¬ìŠ¤íŠ¸")
    st.dataframe(
        df,
        column_config={
            "ë§í¬": st.column_config.LinkColumn("ë°”ë¡œê°€ê¸°", display_text="ğŸ”— ê²Œì‹œíŒ ì´ë™"),
            "D-Day": st.column_config.TextColumn("ìƒíƒœ"),
            "ì¶œì²˜": st.column_config.TextColumn("ê¸°ê´€ëª…", width="medium"),
            "ê³µê³ ëª…": st.column_config.TextColumn("ê³µê³  ì œëª©", width="large")
        },
        hide_index=True,
        use_container_width=True
    )

    st.divider()
    st.subheader("ğŸ“Œ ìƒì„¸ ì •ë³´ ì¹´ë“œ")
    for index, row in df.iterrows():
        icon = "ğŸ“„"
        if row['D-Day'] == "D-Day" or (str(row['D-Day']).startswith("D-") and int(row['D-Day'].split("-")[1]) <= 7):
            icon = "ğŸ”¥"
        elif "í™•ì¸í•„ìš”" in str(row['ë§ˆê°ì¼']):
            icon = "ğŸ”’" # ë³´ì•ˆ/í™•ì¸í•„ìš” ì•„ì´ì½˜
            
        with st.expander(f"{icon} {row['ê³µê³ ëª…']}"):
            st.write(f"**ê¸°ê´€:** {row['ì¶œì²˜']}")
            st.write(f"**ë§ˆê°:** {row['ë§ˆê°ì¼']}")
            if "í™•ì¸í•„ìš”" in str(row['ë§ˆê°ì¼']):
                st.warning("âš ï¸ ì´ ì‚¬ì´íŠ¸ëŠ” ë³´ì•ˆ ì •ì±…ìƒ ë¡œë´‡ ì ‘ê·¼ì„ ë§‰ê³  ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ë§í¬ë¥¼ ëˆŒëŸ¬ ì§ì ‘ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.")
            st.markdown(f"**[ê²Œì‹œíŒ ë°”ë¡œê°€ê¸°]({row['ë§í¬']})**")
else:
    st.error("ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

st.caption(f"ìµœì¢… ì—…ë°ì´íŠ¸: {update_time}")
